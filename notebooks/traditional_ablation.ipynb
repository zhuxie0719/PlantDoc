{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统方法消融实验\n",
    "\n",
    "本notebook进行传统方法的扩展实验，包括：\n",
    "\n",
    "1. **特征组合消融**：HOG单独 vs. HOG+颜色直方图 vs. HOG+LBP vs. 全部组合\n",
    "2. **分类器对比**：SVM vs. Logistic Regression vs. MLP\n",
    "3. **降维策略**：PCA降维 vs. 不降维\n",
    "4. **样本不平衡处理**：类别权重 vs. 无权重\n",
    "\n",
    "**注意**：这些实验可能需要较长时间运行，建议在完成E1基线后再进行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:30.503525Z",
     "start_time": "2025-11-25T09:10:29.935830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "if REPO_ROOT.name == \"notebooks\":\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "SRC_DIR = (REPO_ROOT / \"src\").resolve()\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from data.traditional_loader import load_all_splits\n",
    "from models.traditional_features import (\n",
    "    extract_hog_features,\n",
    "    extract_color_histogram,\n",
    "    extract_lbp_features,\n",
    "    combine_features,\n",
    ")\n",
    "from models.traditional_classifiers import (\n",
    "    train_svm,\n",
    "    train_logistic_regression,\n",
    "    train_mlp,\n",
    "    evaluate_model,\n",
    ")\n",
    "from eval.traditional_eval import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_feature_comparison,\n",
    "    plot_classifier_comparison,\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\", \"Arial Unicode MS\", \"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "SPLIT_JSON = DATA_DIR / \"splits\" / \"plantdoc_split_seed42.json\"\n",
    "PROCESSED_ROOT = DATA_DIR / \"processed\" / \"plantdoc_224\"\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "FIGURES_DIR = OUTPUTS_DIR / \"figures\"\n",
    "LOGS_DIR = OUTPUTS_DIR / \"logs\"\n",
    "\n",
    "print(\"✓ Setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载数据并提取所有特征类型\n",
    "\n",
    "首先提取所有需要的特征类型，然后进行组合实验。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:36.714947Z",
     "start_time": "2025-11-25T09:10:30.510386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|██████████| 1730/1730 [00:04<00:00, 413.70it/s]\n",
      "Loading val: 100%|██████████| 360/360 [00:00<00:00, 420.58it/s]\n",
      "Loading test: 100%|██████████| 398/398 [00:00<00:00, 430.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: Train=(1730, 256, 256, 3), Val=(360, 256, 256, 3), Test=(398, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "splits_data, class_names = load_all_splits(\n",
    "    split_json_path=SPLIT_JSON,\n",
    "    processed_root=PROCESSED_ROOT,\n",
    "    target_size=(256, 256),\n",
    "    grayscale=False,\n",
    ")\n",
    "\n",
    "X_train, y_train, _ = splits_data[\"train\"]\n",
    "X_val, y_val, _ = splits_data[\"val\"]\n",
    "X_test, y_test, _ = splits_data[\"test\"]\n",
    "\n",
    "print(f\"Data loaded: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:17:34.650153Z",
     "start_time": "2025-11-25T09:10:36.723543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HOG features...\n",
      "Extracting color histogram features...\n",
      "Extracting LBP features...\n",
      "\n",
      "Feature dimensions:\n",
      "  HOG: 34596\n",
      "  Color: 96\n",
      "  LBP: 26\n"
     ]
    }
   ],
   "source": [
    "# Extract all feature types\n",
    "print(\"Extracting HOG features...\")\n",
    "X_train_hog = extract_hog_features(X_train)\n",
    "X_val_hog = extract_hog_features(X_val)\n",
    "X_test_hog = extract_hog_features(X_test)\n",
    "\n",
    "print(\"Extracting color histogram features...\")\n",
    "X_train_color = extract_color_histogram(X_train, bins=32)\n",
    "X_val_color = extract_color_histogram(X_val, bins=32)\n",
    "X_test_color = extract_color_histogram(X_test, bins=32)\n",
    "\n",
    "print(\"Extracting LBP features...\")\n",
    "X_train_lbp = extract_lbp_features(X_train)\n",
    "X_val_lbp = extract_lbp_features(X_val)\n",
    "X_test_lbp = extract_lbp_features(X_test)\n",
    "\n",
    "print(f\"\\nFeature dimensions:\")\n",
    "print(f\"  HOG: {X_train_hog.shape[1]}\")\n",
    "print(f\"  Color: {X_train_color.shape[1]}\")\n",
    "print(f\"  LBP: {X_train_lbp.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征组合消融实验\n",
    "\n",
    "对比不同特征组合的效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:17:41.913075Z",
     "start_time": "2025-11-25T09:17:34.907841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature combinations prepared\n"
     ]
    }
   ],
   "source": [
    "# Define feature combinations\n",
    "feature_combinations = {\n",
    "    \"HOG only\": {\n",
    "        \"train\": X_train_hog,\n",
    "        \"val\": X_val_hog,\n",
    "        \"test\": X_test_hog,\n",
    "    },\n",
    "    \"HOG + Color\": {\n",
    "        \"train\": combine_features({\"hog\": X_train_hog, \"color\": X_train_color}, normalize=True)[0],\n",
    "        \"val\": combine_features({\"hog\": X_val_hog, \"color\": X_val_color}, normalize=True)[0],\n",
    "        \"test\": combine_features({\"hog\": X_test_hog, \"color\": X_test_color}, normalize=True)[0],\n",
    "    },\n",
    "    \"HOG + LBP\": {\n",
    "        \"train\": combine_features({\"hog\": X_train_hog, \"lbp\": X_train_lbp}, normalize=True)[0],\n",
    "        \"val\": combine_features({\"hog\": X_val_hog, \"lbp\": X_val_lbp}, normalize=True)[0],\n",
    "        \"test\": combine_features({\"hog\": X_test_hog, \"lbp\": X_test_lbp}, normalize=True)[0],\n",
    "    },\n",
    "    \"HOG + Color + LBP\": {\n",
    "        \"train\": combine_features({\"hog\": X_train_hog, \"color\": X_train_color, \"lbp\": X_train_lbp}, normalize=True)[0],\n",
    "        \"val\": combine_features({\"hog\": X_val_hog, \"color\": X_val_color, \"lbp\": X_val_lbp}, normalize=True)[0],\n",
    "        \"test\": combine_features({\"hog\": X_test_hog, \"color\": X_test_color, \"lbp\": X_test_lbp}, normalize=True)[0],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Feature combinations prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-25T09:17:41.920723Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: HOG only\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test each feature combination with SVM\n",
    "feature_results = []\n",
    "\n",
    "for feat_name, feat_data in feature_combinations.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {feat_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train_feat = feat_data[\"train\"]\n",
    "    X_val_feat = feat_data[\"val\"]\n",
    "    X_test_feat = feat_data[\"test\"]\n",
    "    \n",
    "    # Train SVM\n",
    "    model = train_svm(X_train_feat, y_train, kernel=\"rbf\", C=1.0, class_weight=\"balanced\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate_model(model, X_val_feat, y_val, class_names, verbose=False)\n",
    "    test_metrics = evaluate_model(model, X_test_feat, y_test, class_names, verbose=False)\n",
    "    \n",
    "    feature_results.append({\n",
    "        \"feature\": feat_name,\n",
    "        \"val_accuracy\": val_metrics[\"accuracy\"],\n",
    "        \"val_f1\": val_metrics[\"macro_f1\"],\n",
    "        \"test_accuracy\": test_metrics[\"accuracy\"],\n",
    "        \"test_f1\": test_metrics[\"macro_f1\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}, Test F1: {test_metrics['macro_f1']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "feature_df = pd.DataFrame(feature_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Feature Combination Results Summary\")\n",
    "print(\"=\"*60)\n",
    "print(feature_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature comparison\n",
    "plot_feature_comparison(\n",
    "    feature_df[\"feature\"].tolist(),\n",
    "    feature_df[\"test_accuracy\"].tolist(),\n",
    "    save_path=FIGURES_DIR / \"feature_combination_comparison.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分类器对比实验\n",
    "\n",
    "使用最佳特征组合，对比不同分类器的性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best feature combination (you can change this based on results)\n",
    "best_feat_name = \"HOG + Color + LBP\"  # Update based on results\n",
    "X_train_best = feature_combinations[best_feat_name][\"train\"]\n",
    "X_val_best = feature_combinations[best_feat_name][\"val\"]\n",
    "X_test_best = feature_combinations[best_feat_name][\"test\"]\n",
    "\n",
    "print(f\"Using feature combination: {best_feat_name}\")\n",
    "print(f\"Feature dimension: {X_train_best.shape[1]}\")\n",
    "\n",
    "# Test different classifiers\n",
    "classifier_results = []\n",
    "\n",
    "# SVM\n",
    "print(\"\\nTraining SVM...\")\n",
    "svm_model = train_svm(X_train_best, y_train, kernel=\"rbf\", C=1.0, class_weight=\"balanced\")\n",
    "svm_metrics = evaluate_model(svm_model, X_test_best, y_test, class_names, verbose=False)\n",
    "classifier_results.append({\n",
    "    \"classifier\": \"SVM (RBF)\",\n",
    "    \"accuracy\": svm_metrics[\"accuracy\"],\n",
    "    \"macro_f1\": svm_metrics[\"macro_f1\"],\n",
    "    \"weighted_f1\": svm_metrics[\"weighted_f1\"],\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = train_logistic_regression(X_train_best, y_train, C=1.0, class_weight=\"balanced\")\n",
    "lr_metrics = evaluate_model(lr_model, X_test_best, y_test, class_names, verbose=False)\n",
    "classifier_results.append({\n",
    "    \"classifier\": \"Logistic Regression\",\n",
    "    \"accuracy\": lr_metrics[\"accuracy\"],\n",
    "    \"macro_f1\": lr_metrics[\"macro_f1\"],\n",
    "    \"weighted_f1\": lr_metrics[\"weighted_f1\"],\n",
    "})\n",
    "\n",
    "# MLP\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp_model = train_mlp(X_train_best, y_train, hidden_layer_sizes=(128, 64), max_iter=500)\n",
    "mlp_metrics = evaluate_model(mlp_model, X_test_best, y_test, class_names, verbose=False)\n",
    "classifier_results.append({\n",
    "    \"classifier\": \"MLP (128, 64)\",\n",
    "    \"accuracy\": mlp_metrics[\"accuracy\"],\n",
    "    \"macro_f1\": mlp_metrics[\"macro_f1\"],\n",
    "    \"weighted_f1\": mlp_metrics[\"weighted_f1\"],\n",
    "})\n",
    "\n",
    "classifier_df = pd.DataFrame(classifier_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classifier Comparison Results\")\n",
    "print(\"=\"*60)\n",
    "print(classifier_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classifier comparison\n",
    "plot_classifier_comparison(\n",
    "    classifier_df[\"classifier\"].tolist(),\n",
    "    {\n",
    "        \"Accuracy\": classifier_df[\"accuracy\"].tolist(),\n",
    "        \"Macro F1\": classifier_df[\"macro_f1\"].tolist(),\n",
    "    },\n",
    "    metric_name=\"Accuracy\",\n",
    "    save_path=FIGURES_DIR / \"classifier_comparison.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 保存所有结果\n",
    "\n",
    "汇总所有消融实验结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "feature_df.to_csv(LOGS_DIR / f\"ablation_features_{timestamp}.csv\", index=False)\n",
    "classifier_df.to_csv(LOGS_DIR / f\"ablation_classifiers_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to outputs/logs/\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Best feature combination: {feature_df.loc[feature_df['test_accuracy'].idxmax(), 'feature']}\")\n",
    "print(f\"Best classifier: {classifier_df.loc[classifier_df['accuracy'].idxmax(), 'classifier']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
